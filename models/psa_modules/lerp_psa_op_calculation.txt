def calc_fracs_indices(x, grid_size, grid_range):
    step_size = (grid_size - 1) / (2 * grid_range)

    # One less index at max so we can gaurantee
    # indices are within bounds after the grid_nl_o.diff() call
    # 1 op
    tmp_x = x + grid_range
    # 1 op
    tmp_x.mul_(step_size)
    # 1 op
    tmp_x_floor = tmp_x.floor()
    # 2 op
    tmp_x_floor.clamp_(0, grid_size - 2)

    indices = tmp_x_floor.long()
    # 1 op
    fracs = tmp_x - tmp_x_floor

    return fracs, indices


def forward(ctx, x, grid_nl_o, grid_size, grid_range, grid_ifeat_offset,
        save_memory):
    fracs, indices = calc_fracs_indices(x, grid_size, grid_range)

    # Flatten our base Lookup Tables
    # make sure to put the nonlinear output as the last axis
    grid_nl_o_shape = grid_nl_o.shape
    grid_nl_o = grid_nl_o.swapaxes(-1, -2).reshape(-1)

    # h-1 (does not scale with input size)
    grid_nl_o_slopes = grid_nl_o.diff()

    # Add custom offest per input feature
    # This way each feature indexes a seperate section within the lookup table

    # 1 op
    indices.add_(grid_ifeat_offset)

    # LERP
    # 4 op
    y = grid_nl_o[indices] + fracs * grid_nl_o_slopes[indices]

    # Save scalars for backward call
    ctx.grid_size = grid_size
    ctx.grid_range = grid_range
    ctx.grid_shape = grid_nl_o_shape
    ctx.save_memory = save_memory

    # If saving memory, we will recompute indices and fracs from input x
    if save_memory:
        ctx.save_for_backward(x, grid_nl_o_slopes, grid_ifeat_offset)
    else:
        ctx.save_for_backward(indices, fracs, grid_nl_o_slopes)